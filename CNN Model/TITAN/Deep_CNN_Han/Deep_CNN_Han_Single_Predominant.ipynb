{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Instrument Recognition Based on Deep CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Librarys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T06:44:50.647599984Z",
     "start_time": "2024-12-05T06:44:50.642170830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess Audio Function\n",
    "def preprocess_audio(file_path,\n",
    "                     target_sr=22050,\n",
    "                     n_fft=1024,\n",
    "                     hop_length=512,\n",
    "                     n_mels=128,\n",
    "                     max_length=None):\n",
    "    \"\"\"\n",
    "    Preprocess an audio file for instrument recognition.\n",
    "    Returns:\n",
    "        np.ndarray: Log-mel spectrogram of shape (n_mels, time_steps)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "\n",
    "        # Normalize the audio signal\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "\n",
    "        # Compute Mel-Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y,\n",
    "                                                         sr=target_sr,\n",
    "                                                         n_fft=n_fft,\n",
    "                                                         hop_length=hop_length,\n",
    "                                                         n_mels=n_mels,\n",
    "                                                         fmax=target_sr / 2)\n",
    "\n",
    "        # Apply logarithm\n",
    "        log_mel_spectrogram = np.log10(mel_spectrogram + 1e-9)  # Add epsilon to avoid log(0)\n",
    "\n",
    "        return log_mel_spectrogram\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def precompute_mel_spectrograms(file_paths, save_dir):\n",
    "    \"\"\"\n",
    "    Precomputes mel-spectrograms for a list of audio files and saves them to disk.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for file_path in tqdm(file_paths, desc=\"Precomputing Mel-Spectrograms\"):\n",
    "        relative_path = os.path.relpath(file_path, start=os.path.commonpath(file_paths))\n",
    "        save_path = os.path.join(save_dir, relative_path) + '.npy'\n",
    "        if not os.path.exists(save_path):\n",
    "            mel_spec = preprocess_audio(file_path)\n",
    "            if mel_spec is not None:\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                np.save(save_path, mel_spec)\n",
    "            else:\n",
    "                print(f\"Failed to process {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization of mel spectrogram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Path to a sample audio file\n",
    "sample_file = './../../../IRMAS/IRMAS-TrainingData/flu/008__[flu][nod][cla]0393__1.wav'  # Replace with your actual file path\n",
    "\n",
    "# Preprocess the audio\n",
    "log_mel_spec = preprocess_audio(sample_file)\n",
    "\n",
    "print(f\"Log-Mel Spectrogram shape: {log_mel_spec.shape}\")\n",
    "\n",
    "# Visualize the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(log_mel_spec,\n",
    "                         sr=22050,\n",
    "                         hop_length=512,\n",
    "                         x_axis='time',\n",
    "                         y_axis='mel',\n",
    "                         cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Log-Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Testing Data Process\n",
    "### Loading Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # For deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Function to clean and process label strings\n",
    "def clean_labels(label_text):\n",
    "    \"\"\"\n",
    "    Cleans and splits raw label text from .txt files.\n",
    "    \"\"\"\n",
    "    labels = [label.strip() for label in label_text.split() if label.strip()]\n",
    "    # Remove any numeric-only labels or unexpected characters\n",
    "    labels = [label for label in labels if not label.isdigit()]\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "class IRMASDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, label_encoder, mel_specs_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for IRMAS data with precomputed mel-spectrograms.\n",
    "        Args:\n",
    "            file_paths (list): List of audio file paths.\n",
    "            labels (list): List of labels (instrument names) for each file.\n",
    "            label_encoder (LabelEncoder): Fitted LabelEncoder.\n",
    "            mel_specs_dir (str): Directory where precomputed mel-spectrograms are stored.\n",
    "            transform (callable): Transform to apply to mel-spectrogram.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels  # List of instrument labels (strings)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.mel_specs_dir = mel_specs_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Compute mel-spectrogram paths\n",
    "        self.mel_spec_paths = [self.get_mel_spec_path(fp) for fp in self.file_paths]\n",
    "\n",
    "    def get_mel_spec_path(self, audio_path):\n",
    "        # Get the relative path from the base data directory\n",
    "        relative_path = os.path.relpath(audio_path, start=os.path.commonpath(self.file_paths))\n",
    "        # Construct the path to the mel-spectrogram\n",
    "        mel_spec_path = os.path.join(self.mel_specs_dir, relative_path) + '.npy'\n",
    "        return mel_spec_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel_spec_path = self.mel_spec_paths[idx]\n",
    "        label = self.labels[idx]  # Single instrument label (string)\n",
    "\n",
    "        # Load precomputed mel-spectrogram\n",
    "        if os.path.exists(mel_spec_path):\n",
    "            mel_spec = np.load(mel_spec_path)\n",
    "        else:\n",
    "            # Handle missing mel-spectrogram files\n",
    "            print(f\"Mel-spectrogram not found at {mel_spec_path}. Using zeros.\")\n",
    "            mel_spec = np.zeros((128, 128))  # Adjust shape if necessary\n",
    "\n",
    "        mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)  # Shape: [1, n_mels, time_frames]\n",
    "\n",
    "        # Apply any transformations\n",
    "        if self.transform:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        # Encode label as integer\n",
    "        label_index = self.label_encoder.transform([label])[0]\n",
    "        target = torch.tensor(label_index, dtype=torch.long)\n",
    "\n",
    "        return mel_spec, target\n",
    "\n",
    "\n",
    "\n",
    "def get_metadata(train_dir, test_dir_part1, test_dir_part2):\n",
    "    \"\"\"\n",
    "    Collects file paths and labels from training and testing directories.\n",
    "    Returns:\n",
    "        train_file_paths (list): List of training audio file paths.\n",
    "        train_labels (list): List of labels for training data.\n",
    "        test_file_paths (list): List of testing audio file paths.\n",
    "        test_labels (list): List of labels for testing data.\n",
    "    \"\"\"\n",
    "    train_file_paths = []\n",
    "    train_labels = []\n",
    "    test_file_paths = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Process Training Data\n",
    "    for subdir in os.listdir(train_dir):\n",
    "        subfolder_path = os.path.join(train_dir, subdir)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "                    train_file_paths.append(file_path)\n",
    "                    # Use the subdirectory name as the predominant instrument label\n",
    "                    train_labels.append(subdir)\n",
    "\n",
    "    # Process Testing Data Part 1 and Part 2\n",
    "    for test_dir in [test_dir_part1, test_dir_part2]:\n",
    "        if not os.path.exists(test_dir):\n",
    "            continue\n",
    "        for file in os.listdir(test_dir):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(test_dir, file)\n",
    "                txt_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "\n",
    "                # Read the labels from the .txt file\n",
    "                if os.path.exists(txt_file_path):\n",
    "                    with open(txt_file_path, \"r\") as txt_file:\n",
    "                        raw_labels = txt_file.read()\n",
    "                        labels = clean_labels(raw_labels)\n",
    "                        if labels:\n",
    "                            # Select the predominant instrument (first label)\n",
    "                            predominant_instrument = labels[0]\n",
    "                            test_file_paths.append(file_path)\n",
    "                            test_labels.append(predominant_instrument)\n",
    "                        else:\n",
    "                            # Exclude samples with no labels\n",
    "                            continue\n",
    "                else:\n",
    "                    # Exclude samples with missing label files\n",
    "                    continue\n",
    "\n",
    "    return train_file_paths, train_labels, test_file_paths, test_labels\n",
    "\n",
    "# Function to filter samples without labels\n",
    "def filter_samples_without_labels(file_paths, labels):\n",
    "    filtered_file_paths = []\n",
    "    filtered_labels = []\n",
    "    for fp, lbl in zip(file_paths, labels):\n",
    "        if lbl and lbl != ['unknown']:\n",
    "            filtered_file_paths.append(fp)\n",
    "            filtered_labels.append(lbl)\n",
    "    return filtered_file_paths, filtered_labels\n",
    "\n",
    "# Directories\n",
    "train_dir = \"./../../../IRMAS/IRMAS-TrainingData\"\n",
    "test_dir_part1 = \"./../../../IRMAS/IRMAS-TestingData-Part1/Part1\"\n",
    "test_dir_part2 = \"./../../../IRMAS/IRMAS-TestingData-Part2/IRTestingData-Part2\"\n",
    "\n",
    "# Extract metadata\n",
    "train_file_paths, train_labels, test_file_paths, test_labels = get_metadata(train_dir, test_dir_part1, test_dir_part2)\n",
    "\n",
    "# Combine all labels from training and testing data\n",
    "all_labels = train_labels + test_labels\n",
    "# Remove duplicates\n",
    "all_label_list = list(set(all_labels))\n",
    "\n",
    "# Fit the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_label_list)\n",
    "\n",
    "# Precompute mel-spectrograms for all data\n",
    "mel_specs_dir = './preprocessed_data'\n",
    "all_file_paths = train_file_paths + test_file_paths\n",
    "precompute_mel_spectrograms(all_file_paths, mel_specs_dir)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_files, val_files, train_labels_split, val_labels_split = train_test_split(\n",
    "    train_file_paths,\n",
    "    train_labels,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building Dataloader for Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Dataset instances\n",
    "train_dataset = IRMASDataset(\n",
    "    file_paths=train_files,\n",
    "    labels=train_labels_split,\n",
    "    label_encoder=label_encoder,\n",
    "    mel_specs_dir=mel_specs_dir\n",
    ")\n",
    "\n",
    "val_dataset = IRMASDataset(\n",
    "    file_paths=val_files,\n",
    "    labels=val_labels_split,\n",
    "    label_encoder=label_encoder,\n",
    "    mel_specs_dir=mel_specs_dir\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization of Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Get a sample from the training dataset\n",
    "sample_mel_spec, sample_label = train_dataset[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert tensor to numpy for visualization\n",
    "sample_mel_spec_np = sample_mel_spec.squeeze(0).numpy()\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(sample_mel_spec_np,\n",
    "                         sr=22050,\n",
    "                         hop_length=512,\n",
    "                         x_axis='time',\n",
    "                         y_axis='mel',\n",
    "                         cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(f'Sample Log-Mel Spectrogram ')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep CNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class InstrumentCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(InstrumentCNN, self).__init__()\n",
    "\n",
    "        # Convolutional Block 1\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Convolutional Block 4\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Global Max Pooling\n",
    "        self.global_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256 * 1 * 1, 1024)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional Block 1\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Convolutional Block 4\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "\n",
    "        # Global Max Pooling\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  # Do not apply sigmoid here\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights using Xavier uniform initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = InstrumentCNN(num_classes=num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current Training Session is Running on: \", device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid activation and binary cross-entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50  # You can adjust this\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Lists to store losses and metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
    "        inputs = inputs.to(device)  # Shape: [batch_size, 1, 128, frames_per_segment]\n",
    "        targets = targets.to(device)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Collect predictions and targets for metric computation\n",
    "        preds = torch.sigmoid(outputs).detach().cpu().numpy()  # Apply sigmoid for probabilities\n",
    "        targets_np = targets.detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets_np)\n",
    "\n",
    "    # Calculate average loss over the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Compute F1 Score\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    # Binarize predictions with a threshold (e.g., 0.5)\n",
    "    all_preds_binary = (all_preds >= 0.5).astype(int)\n",
    "    train_f1 = f1_score(all_targets, all_preds_binary, average='micro', zero_division=0)\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_all_preds = []\n",
    "    val_all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Accumulate loss\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            val_all_preds.append(preds)\n",
    "            val_all_targets.append(targets_np)\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "\n",
    "    # Compute validation F1 Score\n",
    "    val_all_preds = np.vstack(val_all_preds)\n",
    "    val_all_targets = np.vstack(val_all_targets)\n",
    "    val_all_preds_binary = (val_all_preds >= 0.5).astype(int)\n",
    "    val_f1 = f1_score(val_all_targets, val_all_preds_binary, average='micro', zero_division=0)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train F1: {train_f1:.4f} - \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Validation loss decreased. Model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'final_model.pth')\n",
    "print(\"Training completed. Model saved as 'final_model.pth'.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_f1_scores, label='Training F1 Score')\n",
    "plt.plot(val_f1_scores, label='Validation F1 Score')\n",
    "plt.title('Training and Validation F1 Score over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evalation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "class IRMASTestDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, label_encoder,\n",
    "                 target_sr=22050,\n",
    "                 n_fft=1024,\n",
    "                 hop_length=512,\n",
    "                 n_mels=128,\n",
    "                 window_size=3.0,\n",
    "                 hop_size=None,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for test audio files with variable lengths and overlapping windows.\n",
    "\n",
    "        Args:\n",
    "            file_paths (list): List of test audio file paths.\n",
    "            labels (list): List of labels (list of instruments) for each file.\n",
    "            label_encoder (LabelEncoder): Fitted LabelEncoder.\n",
    "            window_size (float): Window size in seconds (same as training window).\n",
    "            hop_size (float): Hop size in seconds (half of window size if None).\n",
    "            transform (callable): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = label_encoder\n",
    "        self.target_sr = target_sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.window_size = window_size\n",
    "        self.hop_size = hop_size if hop_size is not None else window_size / 2\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(file_path, sr=self.target_sr, mono=True)\n",
    "\n",
    "        # Normalize the audio signal\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "\n",
    "        # Calculate the number of samples per window and hop\n",
    "        window_length_samples = int(self.window_size * self.target_sr)\n",
    "        hop_length_samples = int(self.hop_size * self.target_sr)\n",
    "\n",
    "        # Compute the number of windows\n",
    "        total_length = len(y)\n",
    "        num_windows = int(np.ceil((total_length - window_length_samples) / hop_length_samples)) + 1\n",
    "\n",
    "        # Segment the audio into overlapping windows\n",
    "        segments = []\n",
    "        for i in range(num_windows):\n",
    "            start = i * hop_length_samples\n",
    "            end = start + window_length_samples\n",
    "            segment = y[start:end]\n",
    "\n",
    "            # Pad segment if necessary\n",
    "            if len(segment) < window_length_samples:\n",
    "                pad_width = window_length_samples - len(segment)\n",
    "                segment = np.pad(segment, (0, pad_width), mode='constant')\n",
    "\n",
    "            # Compute mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=segment,\n",
    "                sr=self.target_sr,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels,\n",
    "                fmax=self.target_sr / 2\n",
    "            )\n",
    "            # Apply logarithm\n",
    "            mel_spec = np.log(mel_spec + 1e-9)\n",
    "            segments.append(mel_spec)\n",
    "\n",
    "        # Convert list of segments to a tensor\n",
    "        segments = np.array(segments)  # Shape: [num_windows, n_mels, time_frames]\n",
    "        segments = torch.tensor(segments, dtype=torch.float32)\n",
    "\n",
    "        # Add channel dimension\n",
    "        segments = segments.unsqueeze(1)  # Shape: [num_windows, 1, n_mels, time_frames]\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            segments = self.transform(segments)\n",
    "\n",
    "        # Convert labels to multi-hot encoding\n",
    "        label_indices = self.label_encoder.transform(labels)\n",
    "        target = torch.zeros(len(self.label_encoder.classes_), dtype=torch.float32)\n",
    "        target[label_indices] = 1.0\n",
    "\n",
    "        return segments, target, file_path  # Return file_path for identification\n",
    "\n",
    "# Instantiate the test dataset\n",
    "test_dataset = IRMASTestDataset(\n",
    "    file_paths=test_file_paths,\n",
    "    labels=test_labels,\n",
    "    label_encoder=label_encoder,\n",
    "    target_sr=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128,\n",
    "    window_size=3.0,  # Same as training window size\n",
    "    hop_size=1.5,     # Half of the window size\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Batch size of 1 since each item may have different number of segments\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Prediction\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             ConfusionMatrixDisplay, precision_recall_fscore_support,\n",
    "                             accuracy_score, hamming_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Instantiate the test dataset and DataLoader\n",
    "test_dataset = IRMASTestDataset(\n",
    "    file_paths=test_file_paths,\n",
    "    labels=test_labels,\n",
    "    label_encoder=label_encoder,\n",
    "    target_sr=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128,\n",
    "    window_size=3.0,\n",
    "    hop_size=1.5,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Define thresholds\n",
    "thresholds = np.linspace(0.2, 0.8, 13)\n",
    "best_f1 = 0.0\n",
    "best_threshold = 0.5\n",
    "f1_scores = []\n",
    "\n",
    "# Store true labels and predicted labels\n",
    "all_true_labels = []\n",
    "all_predicted_labels = {threshold: [] for threshold in thresholds}\n",
    "\n",
    "# Testing loop\n",
    "for segments, target, file_path in tqdm(test_loader, desc='Testing'):\n",
    "    segments = segments.squeeze(0)\n",
    "    target = target.to(device)\n",
    "    num_segments = segments.size(0)\n",
    "    segments = segments.to(device)\n",
    "\n",
    "    # Collect outputs for all segments\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_segments):\n",
    "            input_segment = segments[i].unsqueeze(0)\n",
    "            logits = model(input_segment)\n",
    "            sigmoid_output = torch.sigmoid(logits)\n",
    "            outputs.append(sigmoid_output.cpu().numpy())\n",
    "\n",
    "    outputs = np.vstack(outputs)\n",
    "\n",
    "    # Aggregate predictions by averaging class-wise\n",
    "    avg_outputs = np.mean(outputs, axis=0)\n",
    "\n",
    "    # Normalize by dividing by the maximum value among classes\n",
    "    if np.max(avg_outputs) > 0:\n",
    "        normalized_outputs = avg_outputs / np.max(avg_outputs)\n",
    "    else:\n",
    "        normalized_outputs = avg_outputs\n",
    "\n",
    "    # Store true labels\n",
    "    true_labels = target.cpu().numpy()\n",
    "    all_true_labels.append(true_labels)\n",
    "\n",
    "    # Apply different thresholds and store predictions\n",
    "    for threshold in thresholds:\n",
    "        predicted_labels = (normalized_outputs >= threshold).astype(int)\n",
    "        all_predicted_labels[threshold].append(predicted_labels)\n",
    "\n",
    "all_true_labels = np.vstack(all_true_labels)\n",
    "\n",
    "# Compute F1 scores for each threshold\n",
    "for threshold in thresholds:\n",
    "    preds = np.vstack(all_predicted_labels[threshold])\n",
    "    f1 = f1_score(all_true_labels, preds, average='micro', zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"Threshold: {threshold:.2f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "        best_preds = preds\n",
    "\n",
    "print(f\"\\nBest Threshold: {best_threshold:.2f}, Best F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# Compute confusion matrices for the best threshold\n",
    "confusion_matrices = multilabel_confusion_matrix(all_true_labels, best_preds)\n",
    "\n",
    "# Display confusion matrices and metrics for each class\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Compute class-wise precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_true_labels,\n",
    "    best_preds,\n",
    "    average=None,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "for idx, (cm, label) in enumerate(zip(confusion_matrices, class_labels)):\n",
    "    print(f\"Class: {label}\")\n",
    "    print(f\"  Precision: {precision[idx]:.4f}\")\n",
    "    print(f\"  Recall:    {recall[idx]:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1[idx]:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "    # Optionally display confusion matrix as a plot\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f'Not {label}', label])\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(f'Confusion Matrix for Class: {label}')\n",
    "    plt.show()\n",
    "\n",
    "# Calculate Exact Match Accuracy (Subset Accuracy)\n",
    "exact_match_accuracy = accuracy_score(all_true_labels, best_preds)\n",
    "print(f\"Exact Match Accuracy (Subset Accuracy): {exact_match_accuracy:.4f}\")\n",
    "\n",
    "# Calculate Hamming Loss and Hamming Accuracy\n",
    "hamming_loss_value = hamming_loss(all_true_labels, best_preds)\n",
    "hamming_accuracy = 1 - hamming_loss_value\n",
    "print(f\"Hamming Accuracy: {hamming_accuracy:.4f}\")\n",
    "\n",
    "# Plot F1 Score vs Threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, f1_scores, marker='o')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute the combined confusion matrix\n",
    "num_classes = all_true_labels.shape[1]\n",
    "combined_confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "for true_labels, predicted_labels in zip(all_true_labels, best_preds):\n",
    "    true_indices = np.where(true_labels == 1)[0]\n",
    "    pred_indices = np.where(predicted_labels == 1)[0]\n",
    "    for i in true_indices:\n",
    "        for j in pred_indices:\n",
    "            combined_confusion_matrix[i][j] += 1\n",
    "\n",
    "# Display the combined confusion matrix\n",
    "confusion_df = pd.DataFrame(combined_confusion_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Combined Confusion Matrix for All Instruments')\n",
    "plt.xlabel('Predicted Instruments')\n",
    "plt.ylabel('True Instruments')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
