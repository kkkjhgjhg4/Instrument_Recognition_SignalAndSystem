{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Instrument Recognition Based on Deep CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Librarys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T09:34:47.365714Z",
     "start_time": "2024-12-10T09:34:47.346253Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio Preprocessing\n",
    "\n",
    "*Here we processed the original audio to get both mel-spectrogram as well as other features.*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Function to preprocess audio and compute mel-spectrogram and additional features\n",
    "def preprocess_audio(file_path,\n",
    "                    target_sr=22050,\n",
    "                    n_fft=1024,\n",
    "                    hop_length=512,\n",
    "                    n_mels=128,\n",
    "                    max_length=3.0):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "\n",
    "        # Normalize the audio signal\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "\n",
    "        # Ensure consistency in audio length\n",
    "        max_length_samples = int(target_sr * max_length)\n",
    "        if len(y) > max_length_samples:\n",
    "            y = y[:max_length_samples]\n",
    "        else:\n",
    "            pad_width = max_length_samples - len(y)\n",
    "            y = np.pad(y, (0, pad_width), mode='constant')\n",
    "\n",
    "        # Compute Mel-Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=y,\n",
    "            sr=target_sr,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            fmax=target_sr / 2\n",
    "        )\n",
    "\n",
    "        # Apply natural logarithm\n",
    "        log_mel_spectrogram = np.log(mel_spectrogram + 1e-9)  # Add epsilon to avoid log(0)\n",
    "\n",
    "        # Extract Additional Features\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "        # Aggregate additional features\n",
    "        # Compute mean and variance for each feature\n",
    "        rms_mean = np.mean(rms, axis=1)\n",
    "        rms_var = np.var(rms, axis=1)\n",
    "\n",
    "        spec_cent_mean = np.mean(spec_cent, axis=1)\n",
    "        spec_cent_var = np.var(spec_cent, axis=1)\n",
    "\n",
    "        spec_bw_mean = np.mean(spec_bw, axis=1)\n",
    "        spec_bw_var = np.var(spec_bw, axis=1)\n",
    "\n",
    "        rolloff_mean = np.mean(rolloff, axis=1)\n",
    "        rolloff_var = np.var(rolloff, axis=1)\n",
    "\n",
    "        zcr_mean = np.mean(zcr, axis=1)\n",
    "        zcr_var = np.var(zcr, axis=1)\n",
    "\n",
    "        # Concatenate all aggregated features into a single feature vector\n",
    "        additional_features = np.concatenate([\n",
    "            rms_mean,\n",
    "            rms_var,\n",
    "            spec_cent_mean,\n",
    "            spec_cent_var,\n",
    "            spec_bw_mean,\n",
    "            spec_bw_var,\n",
    "            rolloff_mean,\n",
    "            rolloff_var,\n",
    "            zcr_mean,\n",
    "            zcr_var\n",
    "        ])\n",
    "\n",
    "        return log_mel_spectrogram, additional_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def save_features(mel_spec, additional_features, save_path_mel, save_path_feat):\n",
    "    \"\"\"\n",
    "    Saves the Mel-spectrogram and additional features as separate .npy files.\n",
    "\n",
    "    Parameters:\n",
    "    - mel_spec (np.ndarray): The log Mel-spectrogram.\n",
    "    - additional_features (np.ndarray): The aggregated additional features.\n",
    "    - save_path_mel (str): File path to save the Mel-spectrogram.\n",
    "    - save_path_feat (str): File path to save the additional features.\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.dirname(save_path_mel), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(save_path_feat), exist_ok=True)\n",
    "\n",
    "    # Save the Mel-spectrogram\n",
    "    np.save(save_path_mel, mel_spec)\n",
    "\n",
    "    # Save the additional features\n",
    "    np.save(save_path_feat, additional_features)\n",
    "    \n",
    "def process_dataset(data_dirs, save_dir_mel, save_dir_feat, max_length=3.0):\n",
    "    \"\"\"\n",
    "    Processes all .wav files in the specified directories, extracts features, and saves them.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dirs (list of str): List of directories containing audio data.\n",
    "    - save_dir_mel (str): Root directory to save Mel-spectrograms.\n",
    "    - save_dir_feat (str): Root directory to save additional features.\n",
    "    - max_length (float): Maximum length of audio in seconds.\n",
    "    \"\"\"\n",
    "    for data_dir in data_dirs:\n",
    "        for root, _, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(file_path, data_dir)\n",
    "\n",
    "                    # Define save paths for Mel-spectrogram and additional features\n",
    "                    # For Mel-spectrogram\n",
    "                    save_path_mel = os.path.join(save_dir_mel, relative_path) + '.npy'\n",
    "                    # For Additional Features\n",
    "                    save_path_feat = os.path.join(save_dir_feat, relative_path) + '_features.npy'\n",
    "\n",
    "                    # Skip processing if both files already exist\n",
    "                    if os.path.exists(save_path_mel) and os.path.exists(save_path_feat):\n",
    "                        print(f\"Already processed: {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # Preprocess audio and extract features\n",
    "                    mel_spec, additional_features = preprocess_audio(file_path, max_length=max_length)\n",
    "\n",
    "                    if mel_spec is not None and additional_features is not None:\n",
    "                        # Save the extracted features\n",
    "                        save_features(mel_spec, additional_features, save_path_mel, save_path_feat)\n",
    "                        print(f\"Processed and saved: {save_path_mel} and {save_path_feat}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to process {file_path}\")\n",
    "                        \n",
    "if __name__ == '__main__':\n",
    "    # Define source directories\n",
    "    train_dir = \"./../../../IRMAS/IRMAS-TrainingData\"\n",
    "    test_dir_part1 = \"./../../../IRMAS/IRMAS-TestingData-Part1/Part1\"\n",
    "    test_dir_part2 = \"./../../../IRMAS/IRMAS-TestingData-Part2/IRTestingData-Part2\"\n",
    "\n",
    "    # List of data directories to process\n",
    "    data_dirs = [train_dir, test_dir_part1, test_dir_part2]\n",
    "\n",
    "    # Define directories to save the precomputed features\n",
    "    save_dir_mel = './preprocessed_data_mel_spectrogram'\n",
    "    save_dir_feat = './preprocessed_data_additional_features'\n",
    "\n",
    "    # Process the dataset\n",
    "    process_dataset(data_dirs, save_dir_mel, save_dir_feat)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Testing Data Process\n",
    "### Loading Dataset and Create Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Setup and Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # For deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='data_loading.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Label Cleaning Function\n",
    "# -------------------------------\n",
    "\n",
    "def clean_labels(label_text):\n",
    "    \"\"\"\n",
    "    Cleans and splits raw label text from .txt files.\n",
    "    Handles artifacts like tabs, newlines, and extra spaces.\n",
    "    \"\"\"\n",
    "    labels = [label.strip() for label in label_text.split() if label.strip()]\n",
    "    # Remove any numeric-only labels or unexpected characters\n",
    "    labels = [label for label in labels if not label.isdigit()]\n",
    "    return labels\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Custom Dataset Class\n",
    "# -------------------------------\n",
    "\n",
    "class IRMASDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, label_encoder, mel_specs_dir,\n",
    "                 additional_features_dir, scaler,\n",
    "                 max_length=3.0,\n",
    "                 segment_length=1.0,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for IRMAS data with precomputed mel-spectrograms and additional features.\n",
    "\n",
    "        Args:\n",
    "            file_paths (list): List of audio file paths.\n",
    "            labels (list): List of labels (list of instruments) for each file.\n",
    "            label_encoder (LabelEncoder): Fitted LabelEncoder.\n",
    "            mel_specs_dir (str): Directory where precomputed mel-spectrograms are stored.\n",
    "            additional_features_dir (str): Directory where additional features are stored.\n",
    "            scaler (StandardScaler): Fitted scaler for additional features.\n",
    "            max_length (float): Maximum audio length in seconds.\n",
    "            segment_length (float): Length of each segment in seconds.\n",
    "            transform (callable): Transform to apply to mel-spectrogram.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels  # List of lists of instrument labels\n",
    "        self.label_encoder = label_encoder\n",
    "        self.mel_specs_dir = mel_specs_dir\n",
    "        self.additional_features_dir = additional_features_dir\n",
    "        self.scaler = scaler\n",
    "        self.max_length = max_length\n",
    "        self.segment_length = segment_length\n",
    "        self.transform = transform\n",
    "\n",
    "        self.num_segments = int(self.max_length / self.segment_length)\n",
    "\n",
    "        # Create a mapping from audio file paths to mel-spectrogram and additional features paths\n",
    "        self.mel_spec_paths = [self.get_mel_spec_path(fp) for fp in self.file_paths]\n",
    "        self.additional_feat_paths = [self.get_additional_feat_path(fp) for fp in self.file_paths]\n",
    "\n",
    "    def get_mel_spec_path(self, audio_path):\n",
    "        # Get the relative path from the data directory\n",
    "        relative_path = os.path.relpath(audio_path, start=os.path.commonpath(self.file_paths))\n",
    "        # Construct the path to the mel-spectrogram\n",
    "        mel_spec_path = os.path.join(self.mel_specs_dir, relative_path) + '.npy'\n",
    "        return mel_spec_path\n",
    "\n",
    "    def get_additional_feat_path(self, audio_path):\n",
    "        # Get the relative path from the data directory\n",
    "        relative_path = os.path.relpath(audio_path, start=os.path.commonpath(self.file_paths))\n",
    "        # Construct the path to the additional features\n",
    "        additional_feat_path = os.path.join(self.additional_features_dir, relative_path) + '_features.npy'\n",
    "        return additional_feat_path\n",
    "\n",
    "    def preprocess_additional_features(self, additional_features):\n",
    "        \"\"\"\n",
    "        Applies scaling to the additional features if a scaler is provided.\n",
    "\n",
    "        Args:\n",
    "            additional_features (np.ndarray): Raw additional features.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Scaled additional features.\n",
    "        \"\"\"\n",
    "        if self.scaler:\n",
    "            additional_features = self.scaler.transform(additional_features.reshape(1, -1)).flatten()\n",
    "        return additional_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths) * self.num_segments\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which file and segment this index corresponds to\n",
    "        file_idx = idx // self.num_segments\n",
    "        segment_idx = idx % self.num_segments\n",
    "\n",
    "        mel_spec_path = self.mel_spec_paths[file_idx]\n",
    "        additional_feat_path = self.additional_feat_paths[file_idx]\n",
    "        labels = self.labels[file_idx]  # List of instrument labels for the file\n",
    "\n",
    "        # Load precomputed mel-spectrogram\n",
    "        if os.path.exists(mel_spec_path):\n",
    "            mel_spec = np.load(mel_spec_path)\n",
    "        else:\n",
    "            # Handle missing mel-spectrogram files\n",
    "            logging.warning(f\"Mel-spectrogram not found at {mel_spec_path}. Using zeros.\")\n",
    "            mel_spec = np.zeros((128, int(self.max_length * 22050 / 512) + 1))\n",
    "\n",
    "        mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)  # Shape: [1, 128, time_frames]\n",
    "\n",
    "        # Load additional features\n",
    "        if os.path.exists(additional_feat_path):\n",
    "            additional_features = np.load(additional_feat_path)\n",
    "        else:\n",
    "            # Handle missing additional features files\n",
    "            logging.warning(f\"Additional features not found at {additional_feat_path}. Using zeros.\")\n",
    "            additional_features = np.zeros(10)  # Assuming 10 additional features (mean and var for 5)\n",
    "\n",
    "        # Apply scaling if scaler is provided\n",
    "        additional_features = self.preprocess_additional_features(additional_features)\n",
    "\n",
    "        additional_features = torch.tensor(additional_features, dtype=torch.float32)  # Shape: [10]\n",
    "\n",
    "        # Calculate segment parameters\n",
    "        total_frames = mel_spec.shape[2]\n",
    "        frames_per_segment = total_frames // self.num_segments\n",
    "\n",
    "        start_frame = segment_idx * frames_per_segment\n",
    "        end_frame = start_frame + frames_per_segment\n",
    "        segment = mel_spec[:, :, start_frame:end_frame]  # Shape: [1, 128, frames_per_segment]\n",
    "\n",
    "        # Handle any leftover frames by padding\n",
    "        if segment.shape[2] < frames_per_segment:\n",
    "            pad_amount = frames_per_segment - segment.shape[2]\n",
    "            segment = F.pad(segment, (0, pad_amount), \"constant\", 0)\n",
    "\n",
    "        # Apply any transformations\n",
    "        if self.transform:\n",
    "            segment = self.transform(segment)\n",
    "\n",
    "        # Convert labels to multi-hot encoding\n",
    "        label_indices = self.label_encoder.transform(labels)\n",
    "        target = torch.zeros(len(self.label_encoder.classes_), dtype=torch.float32)\n",
    "        target[label_indices] = 1.0\n",
    "\n",
    "        return (segment, additional_features), target\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metadata Extraction Function\n",
    "# -------------------------------\n",
    "\n",
    "def get_metadata(train_dir, test_dir_part1, test_dir_part2):\n",
    "    \"\"\"\n",
    "    Collects file paths and labels from training and testing directories.\n",
    "\n",
    "    Returns:\n",
    "        train_file_paths (list): List of training audio file paths.\n",
    "        train_labels (list): List of label lists for training data.\n",
    "        test_file_paths (list): List of testing audio file paths.\n",
    "        test_labels (list): List of label lists for testing data.\n",
    "    \"\"\"\n",
    "    train_file_paths = []\n",
    "    train_labels = []\n",
    "    test_file_paths = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Process Training Data\n",
    "    for subdir in os.listdir(train_dir):\n",
    "        subfolder_path = os.path.join(train_dir, subdir)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "                    train_file_paths.append(file_path)\n",
    "                    train_labels.append([subdir])  # Subfolder name as label list\n",
    "\n",
    "    # Process Testing Data Part 1 and Part 2\n",
    "    for test_dir in [test_dir_part1, test_dir_part2]:\n",
    "        if not os.path.exists(test_dir):\n",
    "            continue\n",
    "        for file in os.listdir(test_dir):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(test_dir, file)\n",
    "                txt_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "                test_file_paths.append(file_path)\n",
    "\n",
    "                # Read the labels from the .txt file\n",
    "                if os.path.exists(txt_file_path):\n",
    "                    with open(txt_file_path, \"r\") as txt_file:\n",
    "                        raw_labels = txt_file.read()\n",
    "                        labels = clean_labels(raw_labels)\n",
    "                        test_labels.append(labels)\n",
    "                else:\n",
    "                    # Handle missing label files\n",
    "                    test_labels.append([\"unknown\"])  # Assign default label\n",
    "\n",
    "    return train_file_paths, train_labels, test_file_paths, test_labels\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Label Encoding and Scaler\n",
    "# -------------------------------\n",
    "\n",
    "def fit_scaler(additional_features_dir, train_file_paths):\n",
    "    \"\"\"\n",
    "    Fits a StandardScaler on the additional features from the training set.\n",
    "\n",
    "    Args:\n",
    "        additional_features_dir (str): Directory containing additional features.\n",
    "        train_file_paths (list): List of training audio file paths.\n",
    "\n",
    "    Returns:\n",
    "        scaler (StandardScaler): Fitted scaler object.\n",
    "    \"\"\"\n",
    "    all_additional_features = []\n",
    "    for fp in tqdm(train_file_paths, desc=\"Fitting scaler on additional features\"):\n",
    "        relative_path = os.path.relpath(fp, start=os.path.commonpath(train_file_paths))\n",
    "        additional_feat_path = os.path.join(additional_features_dir, relative_path) + '_features.npy'\n",
    "        if os.path.exists(additional_feat_path):\n",
    "            features = np.load(additional_feat_path)\n",
    "            all_additional_features.append(features)\n",
    "        else:\n",
    "            # Handle missing files if necessary\n",
    "            logging.warning(f\"Additional features not found for {fp}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    all_additional_features = np.array(all_additional_features)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_additional_features)\n",
    "    return scaler"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Execution of Creating Dataloader",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    # ---------------------------\n",
    "    # 6.1 Define Source Directories\n",
    "    # ---------------------------\n",
    "    train_dir = \"./../../../IRMAS/IRMAS-TrainingData\"\n",
    "    test_dir_part1 = \"./../../../IRMAS/IRMAS-TestingData-Part1/Part1\"\n",
    "    test_dir_part2 = \"./../../../IRMAS/IRMAS-TestingData-Part2/IRTestingData-Part2\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.2 Extract Metadata\n",
    "    # ---------------------------\n",
    "    train_file_paths, train_labels, test_file_paths, test_labels = get_metadata(train_dir, test_dir_part1, test_dir_part2)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.3 Combine and Clean Labels\n",
    "    # ---------------------------\n",
    "    all_labels = train_labels + test_labels\n",
    "    all_label_list = [label for labels in all_labels for label in labels]\n",
    "    # Remove duplicates\n",
    "    all_label_list = list(set(all_label_list))\n",
    "    # Remove 'unknown' label if present\n",
    "    if 'unknown' in all_label_list:\n",
    "        all_label_list.remove('unknown')\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.4 Fit and Save LabelEncoder\n",
    "    # ---------------------------\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_label_list)\n",
    "\n",
    "    # Save the encoder\n",
    "    save_path = os.path.join(os.getcwd(), \"nn_label_encoder.pkl\")\n",
    "    print(f\"Saving LabelEncoder to {save_path}\")\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            pickle.dump(label_encoder, file)\n",
    "        print(\"Label encoder saved successfully.\")\n",
    "        logging.info(\"Label encoder saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving label encoder: {e}\")\n",
    "        logging.error(f\"Error saving label encoder: {e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.5 Split Training Data\n",
    "    # ---------------------------\n",
    "    # To perform stratified splitting, convert labels to single labels if possible.\n",
    "    # Since labels are multi-label, stratification is non-trivial. Here, we'll use a simple split.\n",
    "    # For better stratification in multi-label scenarios, consider using iterative stratification.\n",
    "\n",
    "    train_files, val_files, train_labels_split, val_labels_split = train_test_split(\n",
    "        train_file_paths,\n",
    "        train_labels,\n",
    "        test_size=0.15,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "        # stratify=train_labels  # Not directly applicable for multi-label\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_files)}\")\n",
    "    print(f\"Validation samples: {len(val_files)}\")\n",
    "    logging.info(f\"Training samples: {len(train_files)}\")\n",
    "    logging.info(f\"Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.6 Define Feature Directories\n",
    "    # ---------------------------\n",
    "    mel_specs_dir = './preprocessed_data_mel_spectrogram'\n",
    "    additional_features_dir = './preprocessed_data_additional_features'\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.7 Fit Scaler on Training Additional Features\n",
    "    # ---------------------------\n",
    "    scaler = fit_scaler(additional_features_dir, train_files)\n",
    "\n",
    "    # Save the scaler\n",
    "    scaler_save_path = os.path.join(os.getcwd(), \"additional_features_scaler.pkl\")\n",
    "    print(f\"Saving scaler to {scaler_save_path}\")\n",
    "    try:\n",
    "        with open(scaler_save_path, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        print(\"Additional features scaler saved successfully.\")\n",
    "        logging.info(\"Additional features scaler saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving scaler: {e}\")\n",
    "        logging.error(f\"Error saving scaler: {e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.8 Create Dataset Instances\n",
    "    # ---------------------------\n",
    "    train_dataset = IRMASDataset(\n",
    "        file_paths=train_files,\n",
    "        labels=train_labels_split,\n",
    "        label_encoder=label_encoder,\n",
    "        mel_specs_dir=mel_specs_dir,\n",
    "        additional_features_dir=additional_features_dir,\n",
    "        scaler=scaler,  # Pass the fitted scaler\n",
    "        max_length=3.0,\n",
    "        segment_length=1.0\n",
    "    )\n",
    "\n",
    "    val_dataset = IRMASDataset(\n",
    "        file_paths=val_files,\n",
    "        labels=val_labels_split,\n",
    "        label_encoder=label_encoder,\n",
    "        mel_specs_dir=mel_specs_dir,\n",
    "        additional_features_dir=additional_features_dir,\n",
    "        scaler=scaler,  # Pass the fitted scaler\n",
    "        max_length=3.0,\n",
    "        segment_length=1.0\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6.9 Define DataLoaders\n",
    "    # ---------------------------\n",
    "    batch_size = 128\n",
    "    num_workers = 8  # Adjust based on your system's capabilities\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"DataLoaders created successfully.\")\n",
    "    logging.info(\"DataLoaders created successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Inspect Dataset",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(2):\n",
    "    (mel, additional_features), target = train_dataset[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Mel-spectrogram shape: {mel.shape}\")  # Expected: [1, 128, frames_per_segment]\n",
    "    print(f\"  Additional features shape: {additional_features.shape}\")  # Expected: [10]\n",
    "    print(f\"  Target: {target}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neural Network"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep CNN Model",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class InstrumentCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(InstrumentCNN, self).__init__()\n",
    "\n",
    "        # Convolutional Block 1\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Convolutional Block 4\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Global Max Pooling\n",
    "        self.global_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256 * 1 * 1, 1024)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Convolutional Block 1\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Convolutional Block 4\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "\n",
    "        # Global Max Pooling\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        features = F.relu(self.fc1(x))\n",
    "        features = self.dropout4(features)\n",
    "        logits = self.fc2(features)\n",
    "\n",
    "        if return_features:\n",
    "            return features, logits\n",
    "        else:\n",
    "            return logits  # Do not apply sigmoid here\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights using Xavier uniform initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T09:35:03.134490Z",
     "start_time": "2024-12-10T09:35:03.107612Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Audio Features MLP"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T09:40:12.645684Z",
     "start_time": "2024-12-10T09:40:12.636951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioFeaturesMLP(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dims=[64, 128], output_dim=256):\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron to process additional audio features.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input features (default: 10).\n",
    "            hidden_dims (list of int): Sizes of hidden layers (default: [64, 128]).\n",
    "            output_dim (int): Dimension of the output features (default: 256).\n",
    "        \"\"\"\n",
    "        super(AudioFeaturesMLP, self).__init__()\n",
    "        layers_list = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers_list.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers_list.append(nn.ReLU())\n",
    "            layers_list.append(nn.Dropout(0.3))\n",
    "            prev_dim = hidden_dim\n",
    "        layers_list.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers_list.append(nn.ReLU())\n",
    "        layers_list.append(nn.Dropout(0.3))\n",
    "        self.mlp = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)  # Shape: [batch_size, output_dim]"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Combination of CNN and MLP"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DualInputModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DualInputModel, self).__init__()\n",
    "        \n",
    "        # Initialize the existing InstrumentCNN\n",
    "        self.instrument_cnn = InstrumentCNN(num_classes=num_classes)\n",
    "        self.instrument_cnn.eval()  # Set to evaluation mode if pretrained weights are used\n",
    "        \n",
    "        # Initialize the MLP for additional features\n",
    "        self.audio_mlp = AudioFeaturesMLP(input_dim=10, hidden_dims=[64, 128], output_dim=256)\n",
    "        \n",
    "        # Combined fully connected layers\n",
    "        self.fc1 = nn.Linear(1024 + 256, 512)  # 1024 from CNN fc1 + 256 from MLP\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Initialize weights for the new layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, mel_input, additional_features):\n",
    "        \"\"\"\n",
    "        Forward pass for the dual-input model.\n",
    "\n",
    "        Args:\n",
    "            mel_input (torch.Tensor): Mel-spectrogram input tensor of shape [batch_size, 1, 128, time_frames].\n",
    "            additional_features (torch.Tensor): Additional audio features tensor of shape [batch_size, 10].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Classification logits of shape [batch_size, num_classes].\n",
    "        \"\"\"\n",
    "        # Extract features from CNN\n",
    "        features, _ = self.instrument_cnn(mel_input, return_features=True)  # Shape: [batch_size, 1024]\n",
    "        \n",
    "        # Extract features from MLP\n",
    "        mlp_features = self.audio_mlp(additional_features)  # Shape: [batch_size, 256]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((features, mlp_features), dim=1)  # Shape: [batch_size, 1280]\n",
    "        \n",
    "        # Pass through combined fully connected layers\n",
    "        x = self.fc1(combined_features)  # Shape: [batch_size, 512]\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)  # Shape: [batch_size, num_classes]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights for the new fully connected layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 1. Load Label Encoder and Scaler\n",
    "# -------------------------------\n",
    "\n",
    "# Load the LabelEncoder\n",
    "with open(\"nn_label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load the Scaler\n",
    "with open(\"additional_features_scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Instantiate the Model\n",
    "# -------------------------------\n",
    "\n",
    "# Number of instrument classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Initialize the DualInputModel\n",
    "model = DualInputModel(num_classes=num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current Training Session is Running on:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Define Loss Function and Optimizer\n",
    "# -------------------------------\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid activation and binary cross-entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training Parameters\n",
    "# -------------------------------\n",
    "\n",
    "num_epochs = 60  # You can adjust this\n",
    "early_stopping_patience = 20\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Lists to store losses and metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training Loop\n",
    "# -------------------------------\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---------------------------\n",
    "    # 5.1 Training Phase\n",
    "    # ---------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for (mel_segments, additional_features), targets in tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\",\n",
    "        leave=False\n",
    "    ):\n",
    "        # Move inputs to device\n",
    "        mel_segments = mel_segments.to(device)  # Shape: [batch_size, 1, 128, frames_per_segment]\n",
    "        additional_features = additional_features.to(device)  # Shape: [batch_size, 10]\n",
    "        targets = targets.to(device)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(mel_segments, additional_features)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * mel_segments.size(0)\n",
    "\n",
    "        # Collect predictions and targets for metric computation\n",
    "        preds = torch.sigmoid(outputs).detach().cpu().numpy()  # Apply sigmoid for probabilities\n",
    "        targets_np = targets.detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets_np)\n",
    "\n",
    "    # Calculate average training loss over the epoch\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Compute Training F1 Score\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    # Binarize predictions with a threshold (e.g., 0.5)\n",
    "    all_preds_binary = (all_preds >= 0.5).astype(int)\n",
    "    train_f1 = f1_score(all_targets, all_preds_binary, average='micro', zero_division=0)\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5.2 Validation Phase\n",
    "    # ---------------------------\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_all_preds = []\n",
    "    val_all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (mel_segments, additional_features), targets in tqdm(\n",
    "            val_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\",\n",
    "            leave=False\n",
    "        ):\n",
    "            # Move inputs to device\n",
    "            mel_segments = mel_segments.to(device)\n",
    "            additional_features = additional_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(mel_segments, additional_features)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Accumulate loss\n",
    "            val_running_loss += loss.item() * mel_segments.size(0)\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            val_all_preds.append(preds)\n",
    "            val_all_targets.append(targets_np)\n",
    "\n",
    "    # Calculate average validation loss over the epoch\n",
    "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # Compute Validation F1 Score\n",
    "    val_all_preds = np.vstack(val_all_preds)\n",
    "    val_all_targets = np.vstack(val_all_targets)\n",
    "    val_all_preds_binary = (val_all_preds >= 0.5).astype(int)\n",
    "    val_f1 = f1_score(val_all_targets, val_all_preds_binary, average='micro', zero_division=0)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5.3 Print Epoch Statistics\n",
    "    # ---------------------------\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, Train F1: {train_f1:.4f} - \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5.4 Early Stopping Check\n",
    "    # ---------------------------\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_dual_input_model.pth')\n",
    "        print(\"Validation loss decreased. Model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Save the Final Model\n",
    "# -------------------------------\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_dual_input_model.pth'))\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'final_dual_input_model.pth')\n",
    "print(\"Training completed. Model saved as 'final_dual_input_model.pth'.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T09:37:44.580462Z",
     "start_time": "2024-12-10T09:37:44.413761Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'additional_features_scaler.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m     label_encoder \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Load the Scaler\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madditional_features_scaler.pkl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     11\u001B[0m     scaler \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# -------------------------------\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# 2. Instantiate the Model\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# -------------------------------\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Number of instrument classes\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/2024_Autumn_Semester/Signal_System/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    308\u001B[0m     )\n\u001B[0;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'additional_features_scaler.pkl'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_f1_scores, label='Training F1 Score')\n",
    "plt.plot(val_f1_scores, label='Validation F1 Score')\n",
    "plt.title('Training and Validation F1 Score over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evalation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import logging\n",
    "\n",
    "class IRMASTestDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, label_encoder,\n",
    "                 mel_specs_dir,\n",
    "                 additional_features_dir,\n",
    "                 scaler,\n",
    "                 target_sr=22050,\n",
    "                 n_fft=1024,\n",
    "                 hop_length=512,\n",
    "                 n_mels=128,\n",
    "                 window_size=3.0,\n",
    "                 hop_size=None,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for test audio files with variable lengths and overlapping windows.\n",
    "\n",
    "        Args:\n",
    "            file_paths (list): List of test audio file paths.\n",
    "            labels (list): List of labels (list of instruments) for each file.\n",
    "            label_encoder (LabelEncoder): Fitted LabelEncoder.\n",
    "            mel_specs_dir (str): Directory where precomputed mel-spectrograms are stored.\n",
    "            additional_features_dir (str): Directory where additional features are stored.\n",
    "            scaler (StandardScaler): Fitted scaler for additional features.\n",
    "            target_sr (int): Target sampling rate.\n",
    "            n_fft (int): Number of FFT components.\n",
    "            hop_length (int): Number of samples between successive frames.\n",
    "            n_mels (int): Number of Mel bands.\n",
    "            window_size (float): Window size in seconds (same as training window).\n",
    "            hop_size (float): Hop size in seconds (half of window size if None).\n",
    "            transform (callable): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = label_encoder\n",
    "        self.mel_specs_dir = mel_specs_dir\n",
    "        self.additional_features_dir = additional_features_dir\n",
    "        self.scaler = scaler\n",
    "        self.target_sr = target_sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.window_size = window_size\n",
    "        self.hop_size = hop_size if hop_size is not None else window_size / 2\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def get_additional_feat_path(self, audio_path):\n",
    "        # Get the relative path from the data directory\n",
    "        relative_path = os.path.relpath(audio_path, start=os.path.commonpath(self.file_paths))\n",
    "        # Construct the path to the additional features\n",
    "        additional_feat_path = os.path.join(self.additional_features_dir, relative_path) + '_features.npy'\n",
    "        return additional_feat_path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(file_path, sr=self.target_sr, mono=True)\n",
    "\n",
    "        # Normalize the audio signal\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "\n",
    "        # Calculate the number of samples per window and hop\n",
    "        window_length_samples = int(self.window_size * self.target_sr)\n",
    "        hop_length_samples = int(self.hop_size * self.target_sr)\n",
    "\n",
    "        # Compute the number of windows\n",
    "        total_length = len(y)\n",
    "        num_windows = int(np.ceil((total_length - window_length_samples) / hop_length_samples)) + 1\n",
    "\n",
    "        # Segment the audio into overlapping windows\n",
    "        segments = []\n",
    "        for i in range(num_windows):\n",
    "            start = i * hop_length_samples\n",
    "            end = start + window_length_samples\n",
    "            segment = y[start:end]\n",
    "\n",
    "            # Pad segment if necessary\n",
    "            if len(segment) < window_length_samples:\n",
    "                pad_width = window_length_samples - len(segment)\n",
    "                segment = np.pad(segment, (0, pad_width), mode='constant')\n",
    "\n",
    "            # Compute mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=segment,\n",
    "                sr=self.target_sr,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels,\n",
    "                fmax=self.target_sr / 2\n",
    "            )\n",
    "            # Apply logarithm\n",
    "            mel_spec = np.log(mel_spec + 1e-9)\n",
    "            segments.append(mel_spec)\n",
    "\n",
    "        # Convert list of segments to a tensor\n",
    "        segments = np.array(segments)  # Shape: [num_windows, n_mels, time_frames]\n",
    "        segments = torch.tensor(segments, dtype=torch.float32)\n",
    "\n",
    "        # Add channel dimension\n",
    "        segments = segments.unsqueeze(1)  # Shape: [num_windows, 1, n_mels, time_frames]\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            segments = self.transform(segments)\n",
    "\n",
    "        # Load additional features\n",
    "        additional_feat_path = self.get_additional_feat_path(file_path)\n",
    "        if os.path.exists(additional_feat_path):\n",
    "            additional_features = np.load(additional_feat_path)\n",
    "        else:\n",
    "            # Handle missing additional features files\n",
    "            logging.warning(f\"Additional features not found at {additional_feat_path}. Using zeros.\")\n",
    "            additional_features = np.zeros(10)  # Assuming 10 additional features\n",
    "\n",
    "        # Apply scaling if scaler is provided\n",
    "        if self.scaler:\n",
    "            additional_features = self.scaler.transform(additional_features.reshape(1, -1)).flatten()\n",
    "\n",
    "        # Repeat additional_features for each segment\n",
    "        additional_features = np.tile(additional_features, (num_windows, 1))  # Shape: [num_windows, 10]\n",
    "        additional_features = torch.tensor(additional_features, dtype=torch.float32)\n",
    "\n",
    "        # Convert labels to multi-hot encoding\n",
    "        label_indices = self.label_encoder.transform(labels)\n",
    "        target = torch.zeros(len(self.label_encoder.classes_), dtype=torch.float32)\n",
    "        target[label_indices] = 1.0\n",
    "\n",
    "        return (segments, additional_features), target, file_path  # Return file_path for identification"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Prediction\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             ConfusionMatrixDisplay, precision_recall_fscore_support,\n",
    "                             accuracy_score, hamming_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Label Encoder and Scaler\n",
    "# -------------------------------\n",
    "\n",
    "# Load the LabelEncoder\n",
    "with open(\"nn_label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load the Scaler\n",
    "with open(\"additional_features_scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Instantiate the DualInputModel\n",
    "# -------------------------------\n",
    "\n",
    "# Number of instrument classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Initialize the DualInputModel\n",
    "model = DualInputModel(num_classes=num_classes)\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('best_dual_input_model.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Instantiate the Test Dataset and DataLoader\n",
    "# -------------------------------\n",
    "\n",
    "# Define directories for precomputed mel-spectrograms and additional features\n",
    "mel_specs_dir = './preprocessed_data_mel_spectrogram'  # Update as per your directory structure\n",
    "additional_features_dir = './preprocessed_data_additional_features'  # Update as per your directory structure\n",
    "\n",
    "# Instantiate the test dataset\n",
    "test_dataset = IRMASTestDataset(\n",
    "    file_paths=test_file_paths,\n",
    "    labels=test_labels,\n",
    "    label_encoder=label_encoder,\n",
    "    mel_specs_dir=mel_specs_dir,\n",
    "    additional_features_dir=additional_features_dir,\n",
    "    scaler=scaler,\n",
    "    target_sr=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128,\n",
    "    window_size=3.0,  # Same as training window size\n",
    "    hop_size=1.5,     # Half of the window size\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Batch size of 1 since each item may have different number of segments\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Define Thresholds for Prediction\n",
    "# -------------------------------\n",
    "\n",
    "# Define thresholds to evaluate\n",
    "thresholds = np.linspace(0.2, 0.8, 13)\n",
    "best_f1 = 0.0\n",
    "best_threshold = 0.5\n",
    "f1_scores = []\n",
    "\n",
    "# Store true labels and predicted labels for all thresholds\n",
    "all_true_labels = []\n",
    "all_predicted_labels = {threshold: [] for threshold in thresholds}\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Testing Loop\n",
    "# -------------------------------\n",
    "\n",
    "for (segments, additional_features), target, file_path in tqdm(test_loader, desc='Testing'):\n",
    "    segments = segments.squeeze(0)  # Shape: [num_segments, 1, n_mels, time_frames]\n",
    "    additional_features = additional_features.squeeze(0)  # Shape: [num_segments, 10]\n",
    "    target = target.cpu().numpy()  # Shape: [num_classes]\n",
    "    file_path = file_path[0]  # Get the string from list\n",
    "\n",
    "    num_segments = segments.size(0)\n",
    "    segments = segments.to(device)  # [num_segments, 1, n_mels, time_frames]\n",
    "    additional_features = additional_features.to(device)  # [num_segments, 10]\n",
    "\n",
    "    # Forward pass for all segments\n",
    "    with torch.no_grad():\n",
    "        outputs = model(segments, additional_features)  # [num_segments, num_classes]\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()  # [num_segments, num_classes]\n",
    "\n",
    "    # Aggregate predictions by averaging class-wise\n",
    "    avg_probabilities = np.mean(probabilities, axis=0)  # [num_classes]\n",
    "\n",
    "    # Normalize by dividing by the maximum value among classes to avoid scaling issues\n",
    "    if np.max(avg_probabilities) > 0:\n",
    "        normalized_probs = avg_probabilities / np.max(avg_probabilities)\n",
    "    else:\n",
    "        normalized_probs = avg_probabilities\n",
    "\n",
    "    # Store true labels\n",
    "    all_true_labels.append(target)\n",
    "\n",
    "    # Apply different thresholds and store predictions\n",
    "    for threshold in thresholds:\n",
    "        predicted_labels = (normalized_probs >= threshold).astype(int)\n",
    "        all_predicted_labels[threshold].append(predicted_labels)\n",
    "\n",
    "# Convert lists to numpy arrays for metric computation\n",
    "all_true_labels = np.array(all_true_labels)  # Shape: [num_files, num_classes]\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Compute F1 Scores for Each Threshold\n",
    "# -------------------------------\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = np.array(all_predicted_labels[threshold])  # Shape: [num_files, num_classes]\n",
    "    f1 = f1_score(all_true_labels, preds, average='micro', zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"Threshold: {threshold:.2f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "        best_preds = preds\n",
    "\n",
    "print(f\"\\nBest Threshold: {best_threshold:.2f}, Best F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Compute and Display Confusion Matrices for the Best Threshold\n",
    "# -------------------------------\n",
    "\n",
    "# Compute confusion matrices for the best threshold\n",
    "confusion_matrices = multilabel_confusion_matrix(all_true_labels, best_preds)\n",
    "\n",
    "# Display confusion matrices and metrics for each class\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Compute class-wise precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_true_labels,\n",
    "    best_preds,\n",
    "    average=None,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "for idx, (cm, label) in enumerate(zip(confusion_matrices, class_labels)):\n",
    "    print(f\"Class: {label}\")\n",
    "    print(f\"  Precision: {precision[idx]:.4f}\")\n",
    "    print(f\"  Recall:    {recall[idx]:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1[idx]:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "    # Optionally display confusion matrix as a plot\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f'Not {label}', label])\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(f'Confusion Matrix for Class: {label}')\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Compute Overall Metrics\n",
    "# -------------------------------\n",
    "\n",
    "# Calculate Exact Match Accuracy (Subset Accuracy)\n",
    "exact_match_accuracy = accuracy_score(all_true_labels, best_preds)\n",
    "print(f\"Exact Match Accuracy (Subset Accuracy): {exact_match_accuracy:.4f}\")\n",
    "\n",
    "# Calculate Hamming Loss and Hamming Accuracy\n",
    "hamming_loss_value = hamming_loss(all_true_labels, best_preds)\n",
    "hamming_accuracy = 1 - hamming_loss_value\n",
    "print(f\"Hamming Accuracy: {hamming_accuracy:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Plot F1 Score vs Threshold\n",
    "# -------------------------------\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, f1_scores, marker='o')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Compute and Display Combined Confusion Matrix\n",
    "# -------------------------------\n",
    "\n",
    "# Compute the combined confusion matrix\n",
    "num_classes = all_true_labels.shape[1]\n",
    "combined_confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "for true_labels, predicted_labels in zip(all_true_labels, best_preds):\n",
    "    true_indices = np.where(true_labels == 1)[0]\n",
    "    pred_indices = np.where(predicted_labels == 1)[0]\n",
    "    for i in true_indices:\n",
    "        for j in pred_indices:\n",
    "            combined_confusion_matrix[i][j] += 1\n",
    "\n",
    "# Display the combined confusion matrix\n",
    "confusion_df = pd.DataFrame(combined_confusion_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Combined Confusion Matrix for All Instruments')\n",
    "plt.xlabel('Predicted Instruments')\n",
    "plt.ylabel('True Instruments')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
