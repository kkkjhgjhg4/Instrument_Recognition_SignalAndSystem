{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extration",
   "id": "957290e23bfc0220"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pywt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy.stats\n",
    "\n",
    "# Function to extract Mel-Spectrogram and MFCC features\n",
    "def extract_mel_mfcc(file_path, sr=22050, n_mels=128, n_mfcc=20):\n",
    "    y, sr = librosa.load(file_path, sr=sr, mono=True)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec), n_mfcc=n_mfcc)\n",
    "    return mel_spec, mfcc\n",
    "\n",
    "# Function to extract DWT-based statistical features\n",
    "def extract_dwt_features(file_path, wavelet='db4', level=5):\n",
    "    y, sr = librosa.load(file_path, sr=22050, mono=True)\n",
    "    coeffs = pywt.wavedec(y, wavelet, level=level)\n",
    "    features = []\n",
    "    for c in coeffs:\n",
    "        stats = [np.mean(c), np.var(c), np.std(c), \n",
    "                 np.mean(np.abs(c)), np.mean(c**2), \n",
    "                 scipy.stats.skew(c), scipy.stats.entropy(np.abs(c))]\n",
    "        features.extend(stats)\n",
    "    return np.array(features)\n",
    "\n",
    "# Function to process training data\n",
    "def process_training_data(train_dir, save_path):\n",
    "    feature_data = []\n",
    "    labels = []\n",
    "    for subdir in os.listdir(train_dir):\n",
    "        subfolder_path = os.path.join(train_dir, subdir)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in tqdm(os.listdir(subfolder_path), desc=f\"Processing {subdir}\"):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "                    mel_spec, mfcc = extract_mel_mfcc(file_path)\n",
    "                    dwt_features = extract_dwt_features(file_path)\n",
    "                    label = subdir  # Subfolder name as the label\n",
    "                    feature_data.append({'mel_spec': mel_spec, 'mfcc': mfcc, 'dwt': dwt_features})\n",
    "                    labels.append(label)\n",
    "    np.savez(save_path, features=feature_data, labels=labels)\n",
    "\n",
    "# Function to process testing data\n",
    "def process_testing_data(test_dir, save_path):\n",
    "    feature_data = []\n",
    "    labels = []\n",
    "    for file in tqdm(os.listdir(test_dir), desc=\"Processing Test Data\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(test_dir, file)\n",
    "            txt_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "            mel_spec, mfcc = extract_mel_mfcc(file_path)\n",
    "            dwt_features = extract_dwt_features(file_path)\n",
    "\n",
    "            # Read labels from the corresponding .txt file\n",
    "            with open(txt_file_path, \"r\") as txt_file:\n",
    "                file_labels = txt_file.read().splitlines()  # Multi-labels\n",
    "            feature_data.append({'mel_spec': mel_spec, 'mfcc': mfcc, 'dwt': dwt_features})\n",
    "            labels.append(file_labels)\n",
    "    np.savez(save_path, features=feature_data, labels=labels)\n",
    "\n",
    "# Process training and testing data\n",
    "train_dir = \"./../IRMAS/IRMAS-TrainingData\"\n",
    "test_dir_part1 = \"./../IRMAS/IRMAS-TestingData-Part1\"\n",
    "test_dir_part2 = \"./../IRMAS/IRMAS-TestingData-Part2\"\n",
    "\n",
    "# Save paths\n",
    "save_path_train = \"train_features.npz\"\n",
    "save_path_test_part1 = \"test_features_part1.npz\"\n",
    "save_path_test_part2 = \"test_features_part2.npz\"\n",
    "\n",
    "# Process and save\n",
    "process_training_data(train_dir, save_path_train)\n",
    "process_testing_data(test_dir_part1, save_path_test_part1)\n",
    "process_testing_data(test_dir_part2, save_path_test_part2)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation\n",
   "id": "aa063d2dab3330ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Dataset class for IRMAS\n",
    "class IRMASDataset(Dataset):\n",
    "    def __init__(self, feature_file, train=True, transform=None):\n",
    "        # Load features and labels\n",
    "        data = np.load(feature_file, allow_pickle=True)\n",
    "        self.features = data['features']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "        # Encode labels for training data (single-label encoding)\n",
    "        le = LabelEncoder()\n",
    "        if isinstance(self.labels[0], list):  # Multi-label for testing data\n",
    "            self.labels = [le.fit_transform(label) for label in self.labels]\n",
    "        else:  # Single-label for training data\n",
    "            self.labels = le.fit_transform(self.labels)\n",
    "        self.label_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        if train:\n",
    "            self.data, _, self.targets, _ = train_test_split(\n",
    "                self.features, self.labels, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            _, self.data, _, self.targets = train_test_split(\n",
    "                self.features, self.labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        mel_spec = sample['mel_spec']\n",
    "        mfcc = sample['mfcc']\n",
    "        dwt = sample['dwt']\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "            mfcc = self.transform(mfcc)\n",
    "            dwt = self.transform(dwt)\n",
    "\n",
    "        # Convert features to tensors\n",
    "        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)\n",
    "        mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)\n",
    "        dwt_tensor = torch.tensor(dwt, dtype=torch.float32)\n",
    "\n",
    "        return (mel_spec_tensor, mfcc_tensor, dwt_tensor), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloaders(feature_file, batch_size=32):\n",
    "    train_dataset = IRMASDataset(feature_file, train=True)\n",
    "    val_dataset = IRMASDataset(feature_file, train=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, train_dataset.label_mapping\n",
    "\n",
    "# Example usage\n",
    "train_feature_file = \"train_features.npz\"\n",
    "batch_size = 32\n",
    "train_loader, val_loader, label_mapping = create_dataloaders(train_feature_file, batch_size)\n",
    "\n",
    "# Print label mapping\n",
    "print(\"Label mapping:\", label_mapping)"
   ],
   "id": "4ca9c36936f176c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep CNN Model\n",
   "id": "c895e7991e199d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Deep Convolutional Neural Network (CNN)\n",
    "class IRMASModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(IRMASModel, self).__init__()\n",
    "        \n",
    "        # CNN pathway for Mel-Spectrogram and MFCC\n",
    "        self.cnn_path = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
    "        )\n",
    "        \n",
    "        # Dense pathway for DWT-based statistical features\n",
    "        self.dwt_path = nn.Sequential(\n",
    "            nn.Linear(35, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined dense layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mel_spec, mfcc, dwt = x\n",
    "        \n",
    "        # Process Mel-Spectrogram/MFCC features through CNN\n",
    "        mel_spec_out = self.cnn_path(mel_spec)\n",
    "        mfcc_out = self.cnn_path(mfcc)\n",
    "        \n",
    "        # Flatten the outputs\n",
    "        mel_spec_out = mel_spec_out.view(mel_spec_out.size(0), -1)\n",
    "        mfcc_out = mfcc_out.view(mfcc_out.size(0), -1)\n",
    "        \n",
    "        # Combine Mel-Spectrogram and MFCC outputs\n",
    "        combined_cnn_out = mel_spec_out + mfcc_out\n",
    "        \n",
    "        # Process DWT features through Dense Pathway\n",
    "        dwt_out = self.dwt_path(dwt)\n",
    "        \n",
    "        # Combine CNN and DWT features\n",
    "        combined_features = torch.cat((combined_cnn_out, dwt_out), dim=1)\n",
    "        \n",
    "        # Final classification layers\n",
    "        out = self.fc(combined_features)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = 11  # Number of instrument classes in IRMAS dataset\n",
    "model = IRMASModel(num_classes)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ],
   "id": "f862d28df2d6f054"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "b1a9da9ccfabdc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, save_dir):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
    "    \n",
    "    # Move model to the device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track accuracy and loss\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs = [x.to(device) for x in inputs]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs = [x.to(device) for x in inputs]\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save model and history\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f\"model_epoch_{epoch + 1}.pth\"))\n",
    "        torch.save(history, os.path.join(save_dir, \"history.pth\"))\n",
    "\n",
    "    return history\n",
    "\n",
    "# Set up device and directories\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current Training Session is Running on: \", device)\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "train_feature_file = \"train_features.npz\"\n",
    "batch_size = 32\n",
    "train_loader, val_loader, label_mapping = create_dataloaders(train_feature_file, batch_size)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "history = train_model(model, train_loader, val_loader, num_epochs, device, save_dir)"
   ],
   "id": "ccce91b3bd9a5bb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Accuracy and Loss",
   "id": "697075ef7305018c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training history\n",
    "history_path = \"saved_models/history.pth\"\n",
    "history = torch.load(history_path)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3bfc320bbd2154c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testset Evaluation",
   "id": "a4ba8f288062611"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load the test features and labels\n",
    "test_feature_file = \"test_features_part1.npz\"  # Use Part1 or Part2 as needed\n",
    "test_dataset = IRMASDataset(test_feature_file, train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"saved_models/model_epoch_20.pth\"  # Use the best epoch\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs = [x.to(device) for x in inputs]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate performance metrics\n",
    "precision = precision_score(all_labels, all_preds, average='macro')\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=list(label_mapping.keys())))"
   ],
   "id": "2b986bc24a253889"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
